# -*- coding: utf-8 -*-
"""Mcdonalds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mh1NZZR6cr0LNrIM2c4NAhAaSVH9H9UU
"""

pip install Bio

pip install biopython

import pandas as pd
from Bio import SeqIO
from Bio.SeqRecord import SeqRecord
from Bio.Align import MultipleSeqAlignment

# Load the CSV dataset using pandas
df = pd.read_csv("/content/mcdonalds.csv")

# Convert the DataFrame to a list of SeqRecord objects
records = [SeqRecord(seq) for seq in df.values]

# Create a MultipleSeqAlignment object
alignment = MultipleSeqAlignment(records)

# Print sequence IDs
print([record.id for record in alignment])

df.head()

df.shape

MD_x = df.iloc[:, 0:11].copy()

# Convert "Yes" to 1 and "No" to 0
MD_x = (MD_x == "Yes").astype(int)

# Round and calculate column means
result = MD_x.mean().round(2)

print(result)

from sklearn.decomposition import PCA


# Assuming MD_x is the binary matrix from the previous code snippet
MD_pca = PCA()
MD_pca_result = MD_pca.fit_transform(MD_x)

# Display summary statistics
explained_variance_ratio = MD_pca.explained_variance_ratio_
cumulative_explained_variance = explained_variance_ratio.cumsum()

# Display summary information
summary_info = pd.DataFrame({
    'PC': range(1, len(explained_variance_ratio) + 1),
    'Explained Variance Ratio': explained_variance_ratio,
    'Cumulative Explained Variance': cumulative_explained_variance
})

print(summary_info)

print("Standard deviations (1, ..., p=11):")
print(MD_pca.singular_values_.round(1))

print("\nRotation matrix:")
print(MD_pca.components_.round(1))

print("\nMean of each variable:")
print(MD_pca.mean_.round(1))

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Assuming MD_x is the binary matrix and MD_pca is the PCA object from previous code
MD_pca_result = PCA().fit_transform(MD_x)

# Plot PCA points
plt.scatter(MD_pca_result[:, 0], MD_pca_result[:, 1], color="grey")
plt.title("PCA Plot")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Set seed
np.random.seed(1234)

# Assuming MD_x is the binary matrix
k_range = range(2, 9)
best_silhouette_score = -1
best_kmeans_model = None

# Perform k-means clustering for different k values
for k in k_range:
    silhouette_scores = []
    for _ in range(10):  # nrep = 10
        kmeans_model = KMeans(n_clusters=k)
        labels = kmeans_model.fit_predict(MD_x)
        silhouette_scores.append(silhouette_score(MD_x, labels))

    avg_silhouette_score = np.mean(silhouette_scores)

    # Update the best model if a higher silhouette score is found
    if avg_silhouette_score > best_silhouette_score:
        best_silhouette_score = avg_silhouette_score
        best_kmeans_model = kmeans_model

# Print the result
print("Best number of clusters:", best_kmeans_model.n_clusters)
print("Silhouette Score:", best_silhouette_score)
print("Cluster labels:", best_kmeans_model.labels_)

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# Assuming MD_x is the binary matrix
silhouette_scores = []

# Perform k-means clustering for different k values
for k in k_range:
    kmeans_model = KMeans(n_clusters=k)
    labels = kmeans_model.fit_predict(MD_x)
    silhouette_scores.append(silhouette_score(MD_x, labels))

# Plot silhouette scores against the number of segments
plt.plot(k_range, silhouette_scores, marker='o')
plt.xlabel('Number of Segments')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score vs Number of Segments')
plt.show()

import matplotlib.pyplot as plt

# Extracting the results from the Python equivalent
# (Assuming 'results' is the variable containing the clustering results)
n_clusters_values = [result[0] for result in results]
mean_silhouette_scores = [np.mean(result[1]) for result in results]

# Plotting
plt.plot(n_clusters_values, mean_silhouette_scores, marker='o')
plt.xlabel("Number of Segments")
plt.ylabel("Mean Silhouette Score")
plt.title("Silhouette Scores for Different Numbers of Segments")
plt.show()

import matplotlib.pyplot as plt

# Assuming MD.km28["4"] contains the cluster labels
cluster_labels = MD_x["fattening"]

# Plotting the histogram
plt.hist(cluster_labels, bins=np.arange(0, 2, 0.1), edgecolor='black')  # Specify the bin edges as needed
plt.xlabel("Cluster Labels")
plt.ylabel("Frequency")
plt.title("Histogram of Cluster Labels for Cluster '4'")
plt.xlim(0, 1)  # Set the x-axis limit

# Show the plot
plt.show()

# Assuming MD.km28["4"] contains the cluster labels
MD_k4 = MD_x["fattening"].copy()

from sklearn.mixture import GaussianMixture

# Assuming MD_k4 contains the cluster labels
# Assuming MD.x is your data

# Fit a Gaussian Mixture Model
gmm = GaussianMixture(n_components=len(set(MD_k4)), covariance_type='full', random_state=1234)
gmm.fit(MD_x)

# Predict cluster assignments using the provided labels
predicted_labels = gmm.predict(MD_x)

# Now, you have the predicted cluster assignments in 'predicted_labels'
# You can use these assignments for further analysis or evaluation

import matplotlib.pyplot as plt

# Assuming MD.r4 contains the results from the clustering analysis
# Extract segment numbers and segment stability from MD.r4
segment_numbers = MD_x.index
segment_stability = MD_x["fast"]

# Plotting
plt.plot(segment_numbers, segment_stability, marker='o')
plt.xlabel("Segment Number")
plt.ylabel("Segment Stability")
plt.title("Segment Stability for Each Segment")
plt.ylim(0, 1)  # Set the y-axis limit

# Show the plot
plt.show()

!pip install mixem

import numpy as np
import pandas as pd
from sklearn.mixture import GaussianMixture
#from mixem import EM

# Assuming MD.x is a DataFrame with a binary response variable
# Replace 'response_variable_name' with the actual name of your response variable
response_variable_name = "fast"

# Extract the response variable and convert it to a NumPy array
response_variable = MD_x[response_variable_name].values

# Fit a mixture model with binary responses using mixem
def fit_mixture_model(data, n_components):
    initial_params = [{'prob': 1 / n_components, 'params': GaussianMixture(data)} for _ in range(n_components)]
    em = GaussianMixture(data, initial_params)
    em.fit()
    return em

# Set seed for reproducibility
np.random.seed(1234)

# Number of components to consider
n_components_range = range(2, 9)

# Number of repetitions
n_reps = 10

# Dictionary to store results
results = {}

for n_components in n_components_range:
    best_em = None
    best_likelihood = float('-inf')

    for _ in range(n_reps):
        # Fit the mixture model
        em = fit_mixture_model(response_variable, n_components)

        # Update the best model if likelihood is higher
        if em.likelihood > best_likelihood:
            best_likelihood = em.likelihood
            best_em = em

    results[n_components] = best_em

# Display the results
for n_components, em_result in results.items():
    print(f"Number of Components: {n_components}")
    print(em_result.params)

# Note: The actual output may contain additional information about the fitted models.

import matplotlib.pyplot as plt

# Assuming 'results' is the dictionary containing the fitted mixture models
# and 'n_components_range' is the range of components considered

# Extract AIC values
aic_values = [em_result.aic for em_result in results]

# Plotting
plt.plot(n_components_range, aic_values, marker='o')
plt.xlabel("Number of Components")
plt.ylabel("AIC Value")
plt.title("AIC Values for Different Numbers of Components")
plt.show()

import pandas as pd

# Assuming clusters(MD.k4) and clusters(MD.m4) are the cluster assignments
# Replace them with the actual cluster assignments from your analysis

# Assuming MD_k4 contains the cluster labels from k-means
# Assuming MD_m4 contains the cluster labels from the fitted mixture model

# Create a DataFrame with the cluster assignments
df = pd.DataFrame({
    'kmeans': clusters(MD_k4),
    'mixture': clusters(MD_m4)
})

# Create a contingency table
contingency_table = pd.crosstab(df['kmeans'], df['mixture'])

# Display the table
print(contingency_table)

import pandas as pd
from flexmix import flexmix

# Assuming MD_k4 contains the cluster labels from k-means
# Assuming MD.x is your data

# Create a DataFrame with the cluster assignments
df = pd.DataFrame({
    'kmeans': clusters(MD_k4),
    'mixture': MD_m4a.predict_proba(MD.x).argmax(axis=1) + 1
})

# Create a contingency table
contingency_table = pd.crosstab(df['kmeans'], df['mixture'])

# Display the table
print(contingency_table)

# Assuming MD_m4a and MD_m4 are the fitted mixture models
log_likelihood_m4a = MD_m4a.logLik()
log_likelihood_m4 = MD_m4.logLik()

print(f"Log Likelihood (MD_m4a): {log_likelihood_m4a}")
print(f"Log Likelihood (MD_m4): {log_likelihood_m4}")

import numpy as np
import pandas as pd

# Assuming 'mcdonalds' is your DataFrame and 'Like' is a column in the DataFrame
# You can reverse the order of the table as follows:

table_like = pd.value_counts(mcdonalds['Like'])
reversed_table_like = np.flip(table_like)

# Display the reversed table
print(reversed_table_like)

import pandas as pd

# Assuming 'mcdonalds' is your DataFrame and 'Like' is a column in the DataFrame
# Create a new variable 'Like.n'
mcdonalds['Like.n'] = 6 - pd.to_numeric(mcdonalds['Like'], errors='coerce')

# Display the table of 'Like.n'
table_like_n = pd.value_counts(mcdonalds['Like.n'])
print(table_like_n)

import patsy

# Assuming 'mcdonalds' is your DataFrame and 'Like.n' is one of the columns
# Extract the names of the first 11 columns
column_names = mcdonalds.columns[:11]

# Concatenate the variable names into a formula string
formula_str = "Like.n ~ " + " + ".join(column_names)

# Create a formula object
formula = patsy.dmatrix(formula_str, data=mcdonalds, return_type='dataframe')

# Display the formula
print(formula)